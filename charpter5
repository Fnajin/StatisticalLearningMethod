'''
决策树 decision tree
    可以认为是if-then规则的集合
    也可以认为是定义在特征空间与类空间上的条件概率分布。
    根据损失函数最小化原则建立决策树
    
    学习步骤包括 特征选择 决策树的生成和决策树的剪枝
    Quinlan 1986 ID3算法  1993的C4.5算法
    Breiman等  1984 CART算法
    
    决策树模型：
        决策树有结点node和有向边directed edge组成
        内部结点internal node  表示一个特征或属性
        叶结点leaf node 表示一个类
    决策树可以看成一个if-then规则的集合
    决策树的路径：互斥并且完备
    
    决策树与条件概率分布
        决策树还表示给定特征条件下类的条件概率分布；
        X表示特征的随机变量，Y表示类的随机变量，则P(Y|X),X取值于给定划分下单元的集合，
        Y取值于类的集合。
        各叶结点上的条件概率往往偏向某一个类，即属于一个类的概率较大。
    
    决策树学习
        训练数据集{(xi,yi)},
        学习目标是根据给定的训练数据集构成一个决策树模型，是它能能够对实例进行正确的分类
        本质上是从训练数据集中归纳出一组分类规则
        决策数学习的损失函数通常是正则化的极大似然函数，决策树学习的策略是以损失函数
        为目标的最小化。
        次最优sub-optimal
        
       （1）特征选择
       （2）决策树的生成
       （3）决策树的剪枝

    特征选择
        选取对于训练数据具有分类能力的特征，选取准则有信息增益、信息增益比.
        直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割
        成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。
    
    信息增益
        熵entropy-表示随机变量不确定性的度量
        
        条件熵conditional entropy
            设有随机变量(X,Y),其联合概率分布为
                P(X=xi, Y=yj) = pij
            条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量
            X给定的条件下随机变量Y的条件熵H(Y|X)，定义为X给定条件下Y的条件概率分布
            的熵对X的数学期望：
                H(Y|X) = Σ piH(Y|X=xi), 其中pi = P(X=xi)。
            当熵和条件熵中的概率有数据估计(特别是极大似然估计)得到时，所对应的熵
            和条件熵分别称为经验熵empirical entropy
            和经验条件熵empirical conditional entropy,此时，如果有0概率，令0log2(0)=0.
        
        信息增益information gain表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。
        特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵与特征A给定条件下D的经验
        条件熵H(D|A)之差，即
            g(D,A) = H(D) - H(D|A),
        一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息mutual information。
        
        决策树学习中的信息增益等价于训练数据集中类与特征的互信息。
        信息增益表示由于特征A而是的对数据集D的分类的不确定性减少的程度。
        信息增益大的特征具有更强的分类能力
            
    信息增益的算法
        输入：训练数据集D和特征A
        输出：特征A对训练数据集D的信息增益g(D,A)
        (1)计算数据集D的经验熵H(D)
        (2)计算特征A对数据集D的经验条件熵H(D,A)
        (3)计算信息增益
    
    信息增益比 information gain ratio
        g_R(D,A) = g(D,A)/H(D) =  (H(D) - H(D|A))/H(D) = 1 -  H(D|A)/H(D)
            
决策树的生成：
    ID3
        输入:训练数据集D，特征集A，阈值ε
        输出：决策树T
        (1)若D中所有实例属于同一类Ck，则T为单结点数，并将Ck作为该结点的类标记，返回T；
        (2)若A=∅，则T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T
        (3)否则，按算法计算A中各特征对D的信息增益，选择信息增益最大的特征Ag
        (4)如果Ag的信息增益小于阈值ε，则置T为单结点树，并将D中实例数最大的类Ck作为该结点
        的类标记，返回T；
        (5)否则，对Ag的每一可能值ai，依Ag=ai将D分割为若干非空子集Di，将Di中实例数最大
        的类作为标记，构建子结点，有结点及其子结点构成数T，返回T；
        (6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归地调用步(1)~(5),得到
        子树Ti，返回Ti
        <ID3算法只有树的生成，所以该算法生成的数容易产生过拟合>
    C4.5
        输入:训练数据集D，特征集A，阈值ε
        输出：决策树T
        (1)若D中所有实例属于同一类Ck，则T为单结点数，并将Ck作为该结点的类标记，返回T；
        (2)若A=∅，则T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T
        (3)否则，按算法计算A中各特征对D的 信息增益比 ，选择信息增益比最大的特征Ag
        (4)如果Ag的信息增益小于阈值ε，则置T为单结点树，并将D中实例数最大的类Ck作为该结点
        的类标记，返回T；
        (5)否则，对Ag的每一可能值ai，依Ag=ai将D分割为若干非空子集Di，将Di中实例数最大
        的类作为标记，构建子结点，有结点及其子结点构成数T，返回T；
        (6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归地调用步(1)~(5),得到
        子树Ti，返回Ti
    CART
    
    决策树的剪枝-pruning
        剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，
        从而简化分类树模型。
        实现方式：通过极小化决策树整体的损失函数loss function或代价函数cost function
        
        设数T的叶结点个数为|T|,t是树T的叶结点，该叶结点有Nt个样本点，其中第k类的样本点
        有Ntk个，k=1,2,...,K, Ht(T)为叶结点t上的经验熵，α≥0为参数，
        则决策树学习的损失函数可定义为
            Cα(T) = (∑_t Nt*Ht(T)) + α|T|  t=1,2,...,|T|
        其中经验熵为
            Ht(T) = -∑_k (Ntk/Nt)*log2(Ntk/Nt)
            记C(T) = ∑_t Nt*Ht(T) = -∑_t ∑_k Ntk*log2(Ntk/Nt)
            有Cα(T) = C(T) + α|T|
        则C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度；
        |T|表示模型复杂度，参数α≥0控制两者之间的影响
        
    剪枝，就是当α确定时，选择损失函数最小的模型，即损失函数最小的子树。
    当α确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；
    相反，子树越小，模型的复杂度就越低，当往往与训练数据的拟合不好。
    
    决策树生成学习局部的模型，决策树剪枝学习整体的模型。
    
    利用损失函数最小化原则机型剪枝就是用正则化的极大似然估计进行模型的选择。
    
    树的剪枝算法：
        输入：生成算法产生的整个树T，参数α
        输出：休假后的子树Tα
        (1)计算每个结点的经验熵
        (2)递归地从树的叶结点向上回缩
        设一组叶结点回缩到其父结点之前与之后的整体树分别为TB和TA，其对应的损失函数
        值分别是Cα(TB)与Cα(TA),如果
            Cα(TA) ≤ Cα(TB)，
            则进行剪枝，即将父结点变为新的叶结点
        (3)返回(2),直至不能继续为止，得到损失函数最小的子树Tα
'''
#年龄  有工作  有自己的房子  信贷情况  类别
data = [['青年', '否', '否', '一般', '否'],
        ['青年', '否', '否', '好',   '否'],
        ['青年', '是', '否', '好',   '是'],
        ['青年', '是', '是', '一般', '是'],
        ['青年', '否', '否', '一般', '否'],
        
        ['中年', '否', '否', '一般', '否'],
        ['中年', '否', '否', '好',   '否'],
        ['中年', '是', '是', '好',   '是'],
        ['中年', '否', '是', '非常好', '是'],
        ['中年', '否', '是', '非常好', '是'],
        
        ['老年', '否', '是', '非常好', '是'],
        ['老年', '否', '是', '好',   '是'],
        ['老年', '是', '否', '好',   '是'],
        ['老年', '是', '否', '非常好', '是'],
        ['老年', '否', '否', '一般', '否']]
data = np.array(data)
print(data.shape)

def DecisionTreeAlgorithm(features, labels):
    #特征选择
    
    #决策树的生成
    
    #决策树的剪枝
    pass

def Entropy(px):
    #熵-X是一个取有限个值的离散随机变量，其概率分布为P(X=xi)=pi，且sum(pi)=1
    #X的熵定义为H(X) = -Σ(pilog(pi))
    #0<=H(X)<=logn
    if sum(px) != 1:
        return -1
    HX= 0 
    for i in range(len(px)):
        HX += (-1)* px[i] * np.log2(px[i])
    return HX

def ConditionalEntropy(randomVarY, conditionX):
    #条件熵H(Y|X) = Σ piH(Y|X=xi), 其中pi = P(X=xi)
    H_Y_Cond_X = 0
    for i in range(len(conditionX)):
        pi = conditionX[i] #pi = P(X=xi)
        H_Y_Cond_X += pi * 
        
    pass

def InformationGain(featureA, trainDataD):
    #信息增益 g(D,A) = H(D) - H(D|A)， 又称为互信息mutual information
    #给定训练数据集D和特征A，经验熵H(D)表示对数据集D机型分类的不确定性
    #经验条件熵H(D|A)表示在特征A给定的条件下对数据集D机型分类的不确定性。
    gDA = Entropy(trainDataD) - ConditionalEntropy(trainDataD, featureA)
    return gDA

def InformationGainRatio(featureA, trainDataD):
    #g_R(D,A) = g(D,A)/H(D)
    pass
import matplotlib.pyplot as plt
px = [0.5, 0.5]
hx = Entropy(px)
px = np.linspace(0,1,21)
print(px)
hx = []
for i in range(len(px)):
    PX = [px[i], 1-px[i]]
    hx.append(Entropy(PX))
plt.plot(px,hx)
plt.show()




















