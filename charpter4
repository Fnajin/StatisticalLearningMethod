'''
朴素贝叶斯naive Bayes法
    基于贝叶斯定理与特征条件独立假设的分类方法
    对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；
    然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。
    
朴素贝叶斯法的学习与分类
    X是定义在输入空间上的随机变量
    Y是定义在输出空间上的随机变量
      P(X,Y)是X和Y的联合概率分布
  训练数据集{xi,yi}是由P(X,Y)独立同分布产生的。
  
    学习以下先验概率分布和条件概率分布
        先验概率分布P(Y=ck)， k=1，2，...，K
        条件概率分布
            P(X=x|Y=ck) = P(X1=x1, ..., Xn=xn | Y=ck), k=1,2,..,K
    于是学习到联合概率分布P(X,Y)
  
    条件独立性假设：
        P(X=x|Y=ck) = P(X1=x1, ..., Xn=xn | Y=ck)
            = TT P(Xj=xj | Y=ck)
    
    分类时：
        对给定的输入x，通过学习到的模型计算后验概率分布P(Y=ck | X=x),将后验概率
        最大的类作为x的类输出。
        P(Y=ck | X=x) = (P(X=x|Y=ck)P(Y=ck))/(Σ (P(X=x|Y=ck)P(Y=ck)))
        
    P(X|Y) = P(XY)/P(Y)
    P(Y|X) = P(XY)/P(X)
    P(XY) = P(Y)P(X|Y)
    从训练数据集中学习先验概率P(Y=ck)，学习条件概率P(X=x|Y=ck)，
    由此得出P(XY),然后根据P(Y|X) = P(XY)/P(X), 
    而P(X)可由全概率公式得出：P(X) = ΣP(X=x|Y=ck)P(Y=ck),
    
    
    后验概率最大化的含义：
        朴素贝叶斯法将实例分到后验概率最大的类中，等价于期望风险最小化。
        
    
朴素贝叶斯法的参数估计
    极大似然估计（可能会出现所要估计的概率值为0的情况）
        先验概率P(Y=ck)极大似然估计
            P(Y=ck) = Σ I(yi=ck) / N , k=1,2,3,...,K
        设第j个特征xj可能取值的集合为{aj1,aj2, ..., ajSj},条件概率P(Xj=ajl|Y=ck)
        的极大似然估计是：
        P(Xj=ajl|Y=ck) = (Σ I(xij=ajl,yi=ck))/(Σ I(yi=ck))
        j=1,2,...,n
        l=1,2,..,Sj
        k=1,2,...,K
        xij是第i个样本的第j个特征
        ajl是第j的特征可能取的第l个值
        I为指示函数
        
    朴素贝叶斯算法naive Bayes algorithm
        输入{(xi,yi)}
        输出x的分类
        (1)计算先验概率及条件概率
            P(Y=ck);数据中类别为ck的数据量与总数据量的比
            P(Xj=ajl|Y=ck);
        (2)对于给定的实例x，计算P(Y=ck)IIP(Xj=xj|Y=ck)
        (3)确定实例x的类
            y= arg max P(Y=ck)IIP(Xj=xj|Y=ck)
            
            
    贝叶斯估计
        条件概率的贝叶斯估计是在随机变量的各个取值的频数上赋予一个整数λ。
        当λ=0时，是极大似然估计，当λ=1时，称为拉普拉斯平滑Laplace smoothing
        
'''
print('hello')
