'''
感知机perceptron--二分类的线性分类模型
输出空间为Y={+1， -1}。
f(x) = sign(wx+b)
https://zhuanlan.zhihu.com/p/200755375?utm_source=cn.wiz.note

线性分类模型linear classification model
linear classifier线性分类器
分离超平面separating hyperplane
线性可分数据集linearly separable data set
对于所有yi=+1的实例i，有w*xi + b > 0
对于所有yi=-1的实例i，有w*xi + b < 0.

超平面wx+b=0
w:超平面的法向量
b:超平面的截距
假设空间中有一点xo，xo在超平面的投影点为xp，xo到超平安的距离为d，
根据超平面方程wx+b=0，有w*(xp)+b=0,
有超平面的法向量为w，所以向量xo-xp = d*w,xp=xo-dw,
有因为xp在超平面上，所以w*xp +b = w*(xo-dw) + b=0, w*xo +b = w*d*w, d = (w*xo + b)/w*w

误分类点到超平面的距离是
  -yi(w*xi+b)/||w||,
  ||w||是w的L2范数
  所以对于感知机sign(wx+b)学习的损失函数定义为
  L(w,b) = -Σyi(w*xi+b)
  L(w,b)及时感知机学习的经验风险函数
  
  min w,b L(w,b), xi是误分类点集合M的元素，因此感知机学习算法是误分类驱动的，
  具体采用随机梯度下降法（stochastic gradient descent）
  
'''
def SignFunc(x):
    if len(x)==1:
        if x>=0:
            return 1
        else:
            return -1
    else:
        out = copy(x)
    pass
print('hello')

def StochasticGradientDescent(X):
    '''
    首先，任意选取一个超平面w0，b0，然后用梯度下降法不断极小化目标函数，
    极小化过程中不是一直使M中所有误分类点的梯度下降，
    而是一次随机选取一个误分类点使其梯度下降。
    ▽_w L(w,b) = -Σyi*xi , xi∈M
    ▽_b L(w,b) = -Σyi , xi∈M
    随机选取一个误分类点xi,yi,对w和b进行更新：
    w<- w + η*yi*xi
    b<- b + η*yi
    η是步长，称为学习率learning rate
    '''
    pass

'''
感知机学习算法原始形式
输入{xi, yi}, 学习率η（0<η<=1）
输出w,b
感知机模型f(x)=sign(wx+b)
(1)选取初值w0，b0
(2)在训练集中选取数据(xi,yi)
(3)如果yi*(w*xi + b) <=0
  w<- w + η*yi*xi
    b<- b + η*yi
(4)转至(2),直至训练集中没有误分类点
'''
index = 0

def PerceptronModel_Original(X, Y, eta=0.01, w=None, b=None):
    global index
    m = X.shape[0]
    n = X.shape[1]
    #w = np.random.rand((1, n))#[0, 1)
    #if w==None:
    #     w = np.random.randn(1,n)#norm 
    #if b==None:
    #    b = np.random.randn(1,1)
        
    print(w.shape)
    print(b.shape)
    #ErrPredictIndexs=[]
    
    
    while(1):
        cout = 0
    #for i in range(1):
        if(index >= len(X)):
            break
        #print("X[i]",X[index])
        #print("Y[i]",Y[index])
        
        xi = X[index].T#
        yi = Y[index]#
        #print(xi.shape)
        
        #numpy的矩阵星乘(*)为矩阵内各对应位置相乘
        #点乘(.dot)表示求矩阵内积 a.dot(b) 或者np.dot(a,b)
        out = yi * (w.dot(xi) + b)
        #print('out',out)
        if out <=0:
            w = w + eta*xi.T*Y[index]
            b = b + eta*Y[index]
            continue
            #pass
        else:
            print(index, 'cout:',cout)
            cout=0
            index+=1
            break
            #pass
    return w,b

X0 = np.linspace(start=0, stop=10, num=11)
X1 = 2*X0 + 3 + np.random.rand(len(X0))*2
X2 = 2*X0-5  + np.random.rand(len(X0))*2
#print(X0)
#print(X1)
plt.plot(X0, X1)
plt.plot(X0, X2)
X_P = np.array((X0,X1)).T
Y_P = np.ones((X_P.shape[0],1))
X_N = np.array((X0, X2)).T
Y_N = (-1)*np.ones(Y_P.shape)
X = np.vstack((X_P, X_N))
Y = np.vstack((Y_P, Y_N))
#数组或者张量的拼接和合成：#数组a,b:  
#np.vstack((a, b))  #v 表示vertical 垂直，也就是竖着拼接 ,行增加  
#np.hstack((a, b))#h表示Horizontal 横向，列增加
#print(X.shape)
#print(Y)
#print(X)
#plt.show()
import time
ret_w = np.random.randn(1,2)
ret_b = np.random.randn(1,1)

while(1):
    ret_w, ret_b = PerceptronModel_Original(X,Y, eta=0.01, w = ret_w, b= ret_b)
    #print(ret_w, ret_b)
    X_ = np.array([X0])
    #print(X_.shape)
    #print(ret_w.shape)
    #print(ret_w[0,1])
    ret_w = (ret_w/ret_w[0,1])
    #print('ret_w',ret_w)
    #print('X_',X_)
    #print('ret_w[0,0]*X_', ret_w[0,0]*X_)
    y_pre = -(ret_w[0,0]*X_ + ret_b[0,0])
    #print('y_pre',y_pre)
    #print(X_[0,:])
    x_arr = X_[0, :]
    y_arr =  y_pre[0,:]
    #print(x_arr.shape, y_arr.shape)
    plt.plot(X0, X1)
    plt.plot(X0, X2)
    plt.plot(x_arr, y_arr)
    plt.show()
    time.sleep(10)

    
    
print('hello')
'''
感知机学习算法的对偶形式：将w和b表示为实例xi和标记yi的线性组合的形式，通过求解器系数
而求得w和b。不是一般性，将感知机学习算法原始形式中w和b的初始值w0和b0均设为0，
对误分点(xi,yi),通过
w←w+ηyi*xi
b←b+ηyi
逐步修改w，b，设修改n次，则w,b关于(xi, yi)的增量分别为αiyixi 和 αiyi，αi=niη，
所以
w = Σαiyixi
b = Σαiyi
输入：线性可分数据集T{(xi,yi)},yi∈{-1， +1}，xi∈R^n, i=1,2,...,N, 学习率η(0<η<=1)
输出：α，b 感知机模型f(x) = sign(Σαiyixi * x + b)，α=(α1, α2, ..., αN).T
(1)α ← 0, b ← 0
(2)在训练集中选取数据(xi, yi)
(3)如果yi(f(x)) <=0
    αi ← αi + η
    b  ← b + ηyi
(4)转至(2)直到没有误分类数据
'''
